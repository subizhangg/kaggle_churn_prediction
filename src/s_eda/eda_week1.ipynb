{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dependencies\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext()\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import raw data\n",
    "sqlContext = SQLContext(sc)\n",
    "transactions = sqlContext.read.csv('../../data/raw/transactions.csv',header=True)\n",
    "user_logs = sqlContext.read.csv('../../data/raw/user_logs.csv',header=True)\n",
    "train = sqlContext.read.csv('../../data/raw/train.csv',header=True)\n",
    "test = sqlContext.read.csv('../../data/raw/sample_submission_zero.csv',header=True)\n",
    "members = sqlContext.read.csv('../../data/raw/members.csv',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. check the dimension of each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21547746, 9)\n"
     ]
    }
   ],
   "source": [
    "print((transactions.count(), len(transactions.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print((user_logs.count(), len(user_logs.columns))) # it's just too big to even getting two numbers here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(992931, 2)\n"
     ]
    }
   ],
   "source": [
    "print((train.count(), len(train.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(970960, 2)\n"
     ]
    }
   ],
   "source": [
    "print((test.count(), len(test.columns))) # why train set is even as big as test set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5116194, 7)\n"
     ]
    }
   ],
   "source": [
    "print((members.count(), len(members.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it is. Beacuse training set records all users whose service gonna expire in Feb. Test set records all users whose service gonna expire in Mar. It's against common intuition in machine learning data splitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Join training set with the smallest table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = train.join(members, on=['msno'], how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(msno='++4RuqBw0Ss6bQU4oMxaRlbBPoWzoEiIZaxPM04Y4+U=', is_churn='0', city='1', bd='0', gender=None, registered_via='7', registration_init_time='20140714', expiration_date='20170930'),\n",
       " Row(msno='+/HS8LzrRGXolKbxRzDLqrmwuXqPOYixBIPXkyNcKNI=', is_churn='0', city=None, bd=None, gender=None, registered_via=None, registration_init_time=None, expiration_date=None),\n",
       " Row(msno='+/namlXq+u3izRjHCFJV4MgqcXcLidZYszVsROOq/y4=', is_churn='0', city='15', bd='30', gender='male', registered_via='9', registration_init_time='20060603', expiration_date='20170930'),\n",
       " Row(msno='+0/X9tkmyHyet9X80G6GTrDFHnJqvai8d1ZPhayT0os=', is_churn='0', city='9', bd='31', gender='male', registered_via='9', registration_init_time='20040330', expiration_date='20170930'),\n",
       " Row(msno='+09YGn842g6h2EZUXe0VWeC4bBoCbDGfUboitc0vIHw=', is_churn='0', city='15', bd='29', gender='male', registered_via='9', registration_init_time='20080322', expiration_date='20170930'),\n",
       " Row(msno='+0RJtbyhoPAHPa+34MkYcE2Ox0cjMgJOTXMXVBYgkJE=', is_churn='1', city='13', bd='29', gender='female', registered_via='3', registration_init_time='20120612', expiration_date='20170617'),\n",
       " Row(msno='+0jTOa6KGPk1vtNTwRDMZc/McUo41AeuwV3ndo54Y+Q=', is_churn='0', city='5', bd='24', gender='female', registered_via='9', registration_init_time='20140320', expiration_date='20170714'),\n",
       " Row(msno='+0l+FDuhyjaZnu0APnrg5L9QqgaRw4RmdQMvqOtKDmU=', is_churn='0', city='13', bd='31', gender='male', registered_via='3', registration_init_time='20150316', expiration_date='20170923'),\n",
       " Row(msno='+0l/WkoOIugT69NYawwewSLZjIJ17kHIpDdWqcp53RI=', is_churn='0', city='5', bd='0', gender=None, registered_via='3', registration_init_time='20130227', expiration_date='20170922'),\n",
       " Row(msno='+2Df04hr61UUJijMM2xR97gtoQWWDJpnJVKQ7VMYN9o=', is_churn='0', city='6', bd='30', gender='female', registered_via='9', registration_init_time='20080417', expiration_date='20170907')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tables Aggregation : Transaction + user_log  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Joining all features and labels together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Correlation EDA to reduce redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Go to KKBox website and read membership package detai to guess user churn motivation for feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Go through other one's models and learn Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Try simplest models and submit the 1st result "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
